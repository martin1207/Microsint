{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ultralytics\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from ultralytics import YOLO  \n",
    "import val\n",
    "from val import yolo2coco, coco_eval\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes file created at: yolo/datasets_Test-360/one/classes.txt\n",
      "\n",
      "0: 640x1280 13 persons, 1725.5ms\n",
      "Speed: 4.6ms preprocess, 1725.5ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 860.0ms\n",
      "Speed: 4.5ms preprocess, 860.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 740.5ms\n",
      "Speed: 3.7ms preprocess, 740.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 745.5ms\n",
      "Speed: 3.1ms preprocess, 745.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 737.4ms\n",
      "Speed: 3.3ms preprocess, 737.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 746.3ms\n",
      "Speed: 3.4ms preprocess, 746.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 752.0ms\n",
      "Speed: 3.3ms preprocess, 752.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 755.0ms\n",
      "Speed: 3.6ms preprocess, 755.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 752.3ms\n",
      "Speed: 3.2ms preprocess, 752.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 719.5ms\n",
      "Speed: 3.3ms preprocess, 719.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 727.0ms\n",
      "Speed: 3.3ms preprocess, 727.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 748.7ms\n",
      "Speed: 3.1ms preprocess, 748.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 731.7ms\n",
      "Speed: 3.5ms preprocess, 731.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 751.8ms\n",
      "Speed: 3.3ms preprocess, 751.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 748.5ms\n",
      "Speed: 3.3ms preprocess, 748.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 738.5ms\n",
      "Speed: 3.6ms preprocess, 738.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 719.4ms\n",
      "Speed: 3.3ms preprocess, 719.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 727.8ms\n",
      "Speed: 3.8ms preprocess, 727.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 757.8ms\n",
      "Speed: 3.2ms preprocess, 757.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 750.1ms\n",
      "Speed: 3.3ms preprocess, 750.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 744.4ms\n",
      "Speed: 3.1ms preprocess, 744.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 745.1ms\n",
      "Speed: 3.3ms preprocess, 745.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 739.9ms\n",
      "Speed: 4.0ms preprocess, 739.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 762.2ms\n",
      "Speed: 3.4ms preprocess, 762.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 748.8ms\n",
      "Speed: 3.3ms preprocess, 748.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 748.8ms\n",
      "Speed: 3.6ms preprocess, 748.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 710.7ms\n",
      "Speed: 3.3ms preprocess, 710.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 744.1ms\n",
      "Speed: 3.1ms preprocess, 744.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 742.3ms\n",
      "Speed: 3.2ms preprocess, 742.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 738.6ms\n",
      "Speed: 3.1ms preprocess, 738.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 739.4ms\n",
      "Speed: 3.2ms preprocess, 739.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 732.8ms\n",
      "Speed: 3.3ms preprocess, 732.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 745.3ms\n",
      "Speed: 3.3ms preprocess, 745.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 743.0ms\n",
      "Speed: 3.2ms preprocess, 743.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 732.5ms\n",
      "Speed: 3.2ms preprocess, 732.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 797.2ms\n",
      "Speed: 3.2ms preprocess, 797.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 8 persons, 771.3ms\n",
      "Speed: 3.3ms preprocess, 771.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 831.4ms\n",
      "Speed: 3.1ms preprocess, 831.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 9 persons, 771.9ms\n",
      "Speed: 3.3ms preprocess, 771.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 733.9ms\n",
      "Speed: 3.1ms preprocess, 733.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 778.0ms\n",
      "Speed: 3.1ms preprocess, 778.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 751.0ms\n",
      "Speed: 3.9ms preprocess, 751.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 754.1ms\n",
      "Speed: 3.2ms preprocess, 754.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 734.5ms\n",
      "Speed: 3.2ms preprocess, 734.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 737.6ms\n",
      "Speed: 3.2ms preprocess, 737.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 742.1ms\n",
      "Speed: 3.6ms preprocess, 742.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 743.1ms\n",
      "Speed: 3.1ms preprocess, 743.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 723.2ms\n",
      "Speed: 3.3ms preprocess, 723.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 744.1ms\n",
      "Speed: 3.1ms preprocess, 744.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 804.6ms\n",
      "Speed: 3.4ms preprocess, 804.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 822.7ms\n",
      "Speed: 3.8ms preprocess, 822.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 776.0ms\n",
      "Speed: 3.5ms preprocess, 776.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 796.9ms\n",
      "Speed: 3.3ms preprocess, 796.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 9 persons, 751.8ms\n",
      "Speed: 3.5ms preprocess, 751.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 756.4ms\n",
      "Speed: 3.2ms preprocess, 756.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 756.6ms\n",
      "Speed: 3.5ms preprocess, 756.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 758.7ms\n",
      "Speed: 3.2ms preprocess, 758.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 763.9ms\n",
      "Speed: 3.1ms preprocess, 763.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 813.8ms\n",
      "Speed: 3.5ms preprocess, 813.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 805.9ms\n",
      "Speed: 3.4ms preprocess, 805.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 864.3ms\n",
      "Speed: 3.4ms preprocess, 864.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 827.0ms\n",
      "Speed: 3.2ms preprocess, 827.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 763.3ms\n",
      "Speed: 3.1ms preprocess, 763.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 816.7ms\n",
      "Speed: 3.1ms preprocess, 816.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 823.7ms\n",
      "Speed: 3.4ms preprocess, 823.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 797.2ms\n",
      "Speed: 3.2ms preprocess, 797.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 791.4ms\n",
      "Speed: 3.4ms preprocess, 791.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 8 persons, 798.2ms\n",
      "Speed: 3.7ms preprocess, 798.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 791.5ms\n",
      "Speed: 3.5ms preprocess, 791.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 9 persons, 783.3ms\n",
      "Speed: 3.1ms preprocess, 783.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 7 persons, 776.5ms\n",
      "Speed: 3.1ms preprocess, 776.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 8 persons, 782.3ms\n",
      "Speed: 3.9ms preprocess, 782.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 9 persons, 779.0ms\n",
      "Speed: 3.3ms preprocess, 779.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 9 persons, 752.0ms\n",
      "Speed: 3.3ms preprocess, 752.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 9 persons, 753.3ms\n",
      "Speed: 3.2ms preprocess, 753.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 9 persons, 761.9ms\n",
      "Speed: 3.5ms preprocess, 761.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 753.7ms\n",
      "Speed: 3.1ms preprocess, 753.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 9 persons, 801.0ms\n",
      "Speed: 3.2ms preprocess, 801.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 783.0ms\n",
      "Speed: 3.2ms preprocess, 783.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 772.2ms\n",
      "Speed: 3.1ms preprocess, 772.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 818.6ms\n",
      "Speed: 3.7ms preprocess, 818.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 759.1ms\n",
      "Speed: 4.0ms preprocess, 759.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 757.8ms\n",
      "Speed: 3.1ms preprocess, 757.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 758.9ms\n",
      "Speed: 3.6ms preprocess, 758.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 780.4ms\n",
      "Speed: 3.2ms preprocess, 780.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 770.2ms\n",
      "Speed: 3.7ms preprocess, 770.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 753.3ms\n",
      "Speed: 3.1ms preprocess, 753.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 759.2ms\n",
      "Speed: 3.1ms preprocess, 759.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 754.6ms\n",
      "Speed: 3.2ms preprocess, 754.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 747.1ms\n",
      "Speed: 3.1ms preprocess, 747.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 747.8ms\n",
      "Speed: 3.5ms preprocess, 747.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 723.6ms\n",
      "Speed: 3.3ms preprocess, 723.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 745.4ms\n",
      "Speed: 3.1ms preprocess, 745.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 709.6ms\n",
      "Speed: 3.1ms preprocess, 709.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 754.0ms\n",
      "Speed: 3.1ms preprocess, 754.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 746.0ms\n",
      "Speed: 3.1ms preprocess, 746.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 749.6ms\n",
      "Speed: 3.1ms preprocess, 749.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 1 bicycle, 743.8ms\n",
      "Speed: 3.1ms preprocess, 743.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 754.4ms\n",
      "Speed: 3.1ms preprocess, 754.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 751.6ms\n",
      "Speed: 3.1ms preprocess, 751.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 774.1ms\n",
      "Speed: 3.0ms preprocess, 774.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 773.3ms\n",
      "Speed: 3.1ms preprocess, 773.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 762.3ms\n",
      "Speed: 3.1ms preprocess, 762.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 733.2ms\n",
      "Speed: 3.1ms preprocess, 733.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 757.0ms\n",
      "Speed: 3.0ms preprocess, 757.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 762.9ms\n",
      "Speed: 3.2ms preprocess, 762.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 756.0ms\n",
      "Speed: 3.2ms preprocess, 756.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 752.9ms\n",
      "Speed: 3.0ms preprocess, 752.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 1 bicycle, 755.3ms\n",
      "Speed: 3.1ms preprocess, 755.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 741.9ms\n",
      "Speed: 3.7ms preprocess, 741.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 735.0ms\n",
      "Speed: 3.1ms preprocess, 735.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 750.7ms\n",
      "Speed: 3.2ms preprocess, 750.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 747.7ms\n",
      "Speed: 3.6ms preprocess, 747.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 740.2ms\n",
      "Speed: 3.1ms preprocess, 740.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 749.9ms\n",
      "Speed: 3.1ms preprocess, 749.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 747.9ms\n",
      "Speed: 4.0ms preprocess, 747.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 1 bicycle, 760.3ms\n",
      "Speed: 3.1ms preprocess, 760.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 775.5ms\n",
      "Speed: 3.1ms preprocess, 775.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 752.5ms\n",
      "Speed: 3.1ms preprocess, 752.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 754.9ms\n",
      "Speed: 3.1ms preprocess, 754.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 1 bicycle, 761.4ms\n",
      "Speed: 3.1ms preprocess, 761.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 738.1ms\n",
      "Speed: 3.7ms preprocess, 738.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 757.5ms\n",
      "Speed: 3.1ms preprocess, 757.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 756.3ms\n",
      "Speed: 3.1ms preprocess, 756.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 758.5ms\n",
      "Speed: 3.1ms preprocess, 758.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 762.1ms\n",
      "Speed: 3.1ms preprocess, 762.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 749.4ms\n",
      "Speed: 3.1ms preprocess, 749.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 745.6ms\n",
      "Speed: 3.0ms preprocess, 745.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 713.6ms\n",
      "Speed: 3.1ms preprocess, 713.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 746.8ms\n",
      "Speed: 3.1ms preprocess, 746.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 758.7ms\n",
      "Speed: 3.1ms preprocess, 758.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 761.0ms\n",
      "Speed: 3.1ms preprocess, 761.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 745.4ms\n",
      "Speed: 3.1ms preprocess, 745.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 1 bicycle, 741.9ms\n",
      "Speed: 3.1ms preprocess, 741.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 1 bicycle, 749.8ms\n",
      "Speed: 3.1ms preprocess, 749.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 773.1ms\n",
      "Speed: 3.1ms preprocess, 773.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 747.8ms\n",
      "Speed: 3.0ms preprocess, 747.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 750.3ms\n",
      "Speed: 3.2ms preprocess, 750.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 744.0ms\n",
      "Speed: 3.1ms preprocess, 744.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 757.1ms\n",
      "Speed: 3.1ms preprocess, 757.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 757.0ms\n",
      "Speed: 3.1ms preprocess, 757.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 780.9ms\n",
      "Speed: 3.1ms preprocess, 780.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 756.8ms\n",
      "Speed: 3.1ms preprocess, 756.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 784.1ms\n",
      "Speed: 3.1ms preprocess, 784.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 759.5ms\n",
      "Speed: 3.1ms preprocess, 759.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 746.8ms\n",
      "Speed: 3.2ms preprocess, 746.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 769.2ms\n",
      "Speed: 3.1ms preprocess, 769.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 10 persons, 754.7ms\n",
      "Speed: 3.4ms preprocess, 754.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 758.7ms\n",
      "Speed: 3.1ms preprocess, 758.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 746.1ms\n",
      "Speed: 3.2ms preprocess, 746.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 765.4ms\n",
      "Speed: 3.1ms preprocess, 765.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 737.7ms\n",
      "Speed: 3.1ms preprocess, 737.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 745.1ms\n",
      "Speed: 3.6ms preprocess, 745.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 762.4ms\n",
      "Speed: 3.1ms preprocess, 762.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 747.3ms\n",
      "Speed: 3.1ms preprocess, 747.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 772.4ms\n",
      "Speed: 3.1ms preprocess, 772.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 745.9ms\n",
      "Speed: 3.1ms preprocess, 745.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 749.3ms\n",
      "Speed: 3.1ms preprocess, 749.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 764.6ms\n",
      "Speed: 3.1ms preprocess, 764.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 766.4ms\n",
      "Speed: 3.1ms preprocess, 766.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 749.5ms\n",
      "Speed: 3.0ms preprocess, 749.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 753.9ms\n",
      "Speed: 3.1ms preprocess, 753.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 757.8ms\n",
      "Speed: 3.2ms preprocess, 757.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 740.3ms\n",
      "Speed: 3.1ms preprocess, 740.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 1 bicycle, 706.8ms\n",
      "Speed: 3.2ms preprocess, 706.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 731.2ms\n",
      "Speed: 3.2ms preprocess, 731.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 740.6ms\n",
      "Speed: 3.1ms preprocess, 740.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 734.3ms\n",
      "Speed: 3.6ms preprocess, 734.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 745.6ms\n",
      "Speed: 3.0ms preprocess, 745.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 743.8ms\n",
      "Speed: 3.1ms preprocess, 743.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 743.8ms\n",
      "Speed: 3.1ms preprocess, 743.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 752.9ms\n",
      "Speed: 3.0ms preprocess, 752.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 2 bicycles, 767.7ms\n",
      "Speed: 3.0ms preprocess, 767.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 748.4ms\n",
      "Speed: 3.2ms preprocess, 748.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 757.1ms\n",
      "Speed: 3.6ms preprocess, 757.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 746.6ms\n",
      "Speed: 3.0ms preprocess, 746.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 746.8ms\n",
      "Speed: 3.1ms preprocess, 746.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 2 bicycles, 760.5ms\n",
      "Speed: 3.1ms preprocess, 760.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 1 bicycle, 752.2ms\n",
      "Speed: 3.1ms preprocess, 752.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 1 bicycle, 744.4ms\n",
      "Speed: 3.1ms preprocess, 744.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 761.8ms\n",
      "Speed: 3.1ms preprocess, 761.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 2 bicycles, 772.4ms\n",
      "Speed: 3.0ms preprocess, 772.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 739.2ms\n",
      "Speed: 3.6ms preprocess, 739.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 2 bicycles, 741.8ms\n",
      "Speed: 3.1ms preprocess, 741.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 753.2ms\n",
      "Speed: 3.1ms preprocess, 753.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 769.5ms\n",
      "Speed: 3.1ms preprocess, 769.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 748.0ms\n",
      "Speed: 3.1ms preprocess, 748.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 730.6ms\n",
      "Speed: 3.1ms preprocess, 730.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 746.9ms\n",
      "Speed: 3.1ms preprocess, 746.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 757.7ms\n",
      "Speed: 3.1ms preprocess, 757.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 766.1ms\n",
      "Speed: 3.7ms preprocess, 766.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 743.9ms\n",
      "Speed: 3.0ms preprocess, 743.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 1 bicycle, 752.2ms\n",
      "Speed: 3.2ms preprocess, 752.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 756.0ms\n",
      "Speed: 3.1ms preprocess, 756.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 737.9ms\n",
      "Speed: 3.1ms preprocess, 737.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 776.9ms\n",
      "Speed: 3.2ms preprocess, 776.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 1 bicycle, 772.4ms\n",
      "Speed: 3.0ms preprocess, 772.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 753.9ms\n",
      "Speed: 3.7ms preprocess, 753.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 764.8ms\n",
      "Speed: 3.1ms preprocess, 764.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 760.3ms\n",
      "Speed: 3.1ms preprocess, 760.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 11 persons, 1 bicycle, 738.2ms\n",
      "Speed: 4.1ms preprocess, 738.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 780.1ms\n",
      "Speed: 3.1ms preprocess, 780.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 1 bicycle, 831.4ms\n",
      "Speed: 3.1ms preprocess, 831.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 2 bicycles, 796.9ms\n",
      "Speed: 3.6ms preprocess, 796.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 757.2ms\n",
      "Speed: 3.1ms preprocess, 757.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 758.9ms\n",
      "Speed: 3.1ms preprocess, 758.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 734.7ms\n",
      "Speed: 3.2ms preprocess, 734.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 749.8ms\n",
      "Speed: 3.3ms preprocess, 749.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 781.9ms\n",
      "Speed: 3.1ms preprocess, 781.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 714.2ms\n",
      "Speed: 3.1ms preprocess, 714.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 752.5ms\n",
      "Speed: 3.1ms preprocess, 752.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 759.1ms\n",
      "Speed: 3.1ms preprocess, 759.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 747.2ms\n",
      "Speed: 3.1ms preprocess, 747.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 762.8ms\n",
      "Speed: 3.1ms preprocess, 762.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 760.6ms\n",
      "Speed: 3.1ms preprocess, 760.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 711.9ms\n",
      "Speed: 3.0ms preprocess, 711.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 740.0ms\n",
      "Speed: 3.0ms preprocess, 740.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 765.1ms\n",
      "Speed: 3.1ms preprocess, 765.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 760.2ms\n",
      "Speed: 3.1ms preprocess, 760.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 745.6ms\n",
      "Speed: 3.0ms preprocess, 745.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 2 bicycles, 770.1ms\n",
      "Speed: 3.2ms preprocess, 770.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 2 bicycles, 780.3ms\n",
      "Speed: 3.1ms preprocess, 780.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 772.0ms\n",
      "Speed: 3.2ms preprocess, 772.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 754.4ms\n",
      "Speed: 3.0ms preprocess, 754.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 759.9ms\n",
      "Speed: 3.1ms preprocess, 759.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 746.9ms\n",
      "Speed: 3.1ms preprocess, 746.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 840.3ms\n",
      "Speed: 3.4ms preprocess, 840.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 958.6ms\n",
      "Speed: 3.1ms preprocess, 958.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 779.8ms\n",
      "Speed: 3.3ms preprocess, 779.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 756.2ms\n",
      "Speed: 4.0ms preprocess, 756.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 756.1ms\n",
      "Speed: 3.2ms preprocess, 756.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 766.0ms\n",
      "Speed: 3.2ms preprocess, 766.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 2 bicycles, 766.4ms\n",
      "Speed: 3.1ms preprocess, 766.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 765.0ms\n",
      "Speed: 3.1ms preprocess, 765.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 878.9ms\n",
      "Speed: 3.6ms preprocess, 878.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 805.0ms\n",
      "Speed: 3.5ms preprocess, 805.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 731.8ms\n",
      "Speed: 3.1ms preprocess, 731.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 758.3ms\n",
      "Speed: 3.3ms preprocess, 758.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 933.5ms\n",
      "Speed: 3.4ms preprocess, 933.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 860.4ms\n",
      "Speed: 3.1ms preprocess, 860.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 773.0ms\n",
      "Speed: 3.1ms preprocess, 773.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 757.9ms\n",
      "Speed: 3.1ms preprocess, 757.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 747.6ms\n",
      "Speed: 3.2ms preprocess, 747.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 755.8ms\n",
      "Speed: 3.1ms preprocess, 755.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 776.1ms\n",
      "Speed: 3.3ms preprocess, 776.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 20 persons, 2 bicycles, 751.5ms\n",
      "Speed: 3.0ms preprocess, 751.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 779.2ms\n",
      "Speed: 3.1ms preprocess, 779.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 1 bicycle, 752.9ms\n",
      "Speed: 3.1ms preprocess, 752.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 739.4ms\n",
      "Speed: 3.1ms preprocess, 739.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 1 bicycle, 786.9ms\n",
      "Speed: 3.1ms preprocess, 786.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 2 bicycles, 728.2ms\n",
      "Speed: 3.1ms preprocess, 728.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 21 persons, 2 bicycles, 741.0ms\n",
      "Speed: 3.2ms preprocess, 741.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 736.0ms\n",
      "Speed: 3.5ms preprocess, 736.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 759.3ms\n",
      "Speed: 3.6ms preprocess, 759.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 2 bicycles, 764.2ms\n",
      "Speed: 3.1ms preprocess, 764.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 2 bicycles, 754.8ms\n",
      "Speed: 3.2ms preprocess, 754.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 777.1ms\n",
      "Speed: 3.1ms preprocess, 777.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 2 bicycles, 763.5ms\n",
      "Speed: 3.2ms preprocess, 763.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 754.6ms\n",
      "Speed: 3.1ms preprocess, 754.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 20 persons, 2 bicycles, 768.8ms\n",
      "Speed: 3.2ms preprocess, 768.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 745.5ms\n",
      "Speed: 3.1ms preprocess, 745.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 757.1ms\n",
      "Speed: 3.2ms preprocess, 757.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 765.9ms\n",
      "Speed: 3.0ms preprocess, 765.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 759.6ms\n",
      "Speed: 3.2ms preprocess, 759.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 2 bicycles, 757.6ms\n",
      "Speed: 3.1ms preprocess, 757.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 761.8ms\n",
      "Speed: 3.1ms preprocess, 761.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 734.3ms\n",
      "Speed: 3.8ms preprocess, 734.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 735.5ms\n",
      "Speed: 3.1ms preprocess, 735.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 742.2ms\n",
      "Speed: 3.2ms preprocess, 742.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 758.9ms\n",
      "Speed: 3.0ms preprocess, 758.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 2 bicycles, 744.1ms\n",
      "Speed: 3.1ms preprocess, 744.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 708.8ms\n",
      "Speed: 3.1ms preprocess, 708.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 2 bicycles, 741.6ms\n",
      "Speed: 3.1ms preprocess, 741.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 748.3ms\n",
      "Speed: 3.1ms preprocess, 748.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 761.1ms\n",
      "Speed: 3.1ms preprocess, 761.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 750.7ms\n",
      "Speed: 3.2ms preprocess, 750.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 760.2ms\n",
      "Speed: 3.2ms preprocess, 760.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 743.9ms\n",
      "Speed: 3.0ms preprocess, 743.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 739.3ms\n",
      "Speed: 3.1ms preprocess, 739.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 798.2ms\n",
      "Speed: 3.1ms preprocess, 798.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 762.6ms\n",
      "Speed: 3.1ms preprocess, 762.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 759.8ms\n",
      "Speed: 3.0ms preprocess, 759.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 754.0ms\n",
      "Speed: 3.0ms preprocess, 754.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 745.8ms\n",
      "Speed: 3.1ms preprocess, 745.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 776.0ms\n",
      "Speed: 3.6ms preprocess, 776.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 1 bicycle, 744.6ms\n",
      "Speed: 3.1ms preprocess, 744.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 1 bicycle, 769.0ms\n",
      "Speed: 3.1ms preprocess, 769.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 1 bicycle, 771.7ms\n",
      "Speed: 3.1ms preprocess, 771.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 3 bicycles, 782.9ms\n",
      "Speed: 3.1ms preprocess, 782.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 2 bicycles, 749.9ms\n",
      "Speed: 3.1ms preprocess, 749.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 3 bicycles, 750.5ms\n",
      "Speed: 3.6ms preprocess, 750.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 3 bicycles, 719.2ms\n",
      "Speed: 3.2ms preprocess, 719.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 3 bicycles, 706.0ms\n",
      "Speed: 3.1ms preprocess, 706.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 1 bicycle, 710.4ms\n",
      "Speed: 3.0ms preprocess, 710.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 1 bicycle, 770.6ms\n",
      "Speed: 3.1ms preprocess, 770.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 2 bicycles, 769.9ms\n",
      "Speed: 3.1ms preprocess, 769.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 750.0ms\n",
      "Speed: 3.2ms preprocess, 750.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 2 bicycles, 751.0ms\n",
      "Speed: 3.1ms preprocess, 751.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 1 bicycle, 756.8ms\n",
      "Speed: 3.0ms preprocess, 756.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 1 bicycle, 773.0ms\n",
      "Speed: 3.1ms preprocess, 773.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 1 bicycle, 773.7ms\n",
      "Speed: 3.1ms preprocess, 773.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 2 bicycles, 757.6ms\n",
      "Speed: 3.1ms preprocess, 757.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 1 bicycle, 746.6ms\n",
      "Speed: 3.0ms preprocess, 746.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 2 bicycles, 753.3ms\n",
      "Speed: 3.1ms preprocess, 753.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 1 bicycle, 773.4ms\n",
      "Speed: 3.1ms preprocess, 773.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 1 bicycle, 765.5ms\n",
      "Speed: 3.1ms preprocess, 765.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 1 bicycle, 750.4ms\n",
      "Speed: 3.4ms preprocess, 750.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 1 bicycle, 747.2ms\n",
      "Speed: 3.1ms preprocess, 747.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 760.0ms\n",
      "Speed: 3.3ms preprocess, 760.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 745.7ms\n",
      "Speed: 3.2ms preprocess, 745.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 768.0ms\n",
      "Speed: 3.7ms preprocess, 768.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 1 bicycle, 741.5ms\n",
      "Speed: 3.1ms preprocess, 741.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 754.6ms\n",
      "Speed: 3.1ms preprocess, 754.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 1 bicycle, 770.5ms\n",
      "Speed: 3.1ms preprocess, 770.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 1 bicycle, 749.4ms\n",
      "Speed: 3.2ms preprocess, 749.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 1 bicycle, 749.6ms\n",
      "Speed: 3.1ms preprocess, 749.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 758.9ms\n",
      "Speed: 3.1ms preprocess, 758.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 755.5ms\n",
      "Speed: 3.1ms preprocess, 755.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 767.0ms\n",
      "Speed: 3.1ms preprocess, 767.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 774.1ms\n",
      "Speed: 3.1ms preprocess, 774.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 756.5ms\n",
      "Speed: 3.1ms preprocess, 756.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 12 persons, 747.3ms\n",
      "Speed: 3.0ms preprocess, 747.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 748.4ms\n",
      "Speed: 3.1ms preprocess, 748.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 766.7ms\n",
      "Speed: 3.3ms preprocess, 766.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 762.4ms\n",
      "Speed: 3.2ms preprocess, 762.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 703.1ms\n",
      "Speed: 3.0ms preprocess, 703.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 709.7ms\n",
      "Speed: 3.0ms preprocess, 709.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 746.0ms\n",
      "Speed: 3.6ms preprocess, 746.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 737.4ms\n",
      "Speed: 3.1ms preprocess, 737.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 787.2ms\n",
      "Speed: 3.0ms preprocess, 787.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 765.8ms\n",
      "Speed: 3.2ms preprocess, 765.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 777.8ms\n",
      "Speed: 3.4ms preprocess, 777.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 770.4ms\n",
      "Speed: 3.1ms preprocess, 770.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 764.9ms\n",
      "Speed: 3.0ms preprocess, 764.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 752.3ms\n",
      "Speed: 3.1ms preprocess, 752.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 772.2ms\n",
      "Speed: 3.2ms preprocess, 772.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 747.7ms\n",
      "Speed: 3.7ms preprocess, 747.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 791.7ms\n",
      "Speed: 3.0ms preprocess, 791.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 765.8ms\n",
      "Speed: 3.0ms preprocess, 765.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 749.2ms\n",
      "Speed: 3.3ms preprocess, 749.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 744.9ms\n",
      "Speed: 3.4ms preprocess, 744.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 753.1ms\n",
      "Speed: 3.1ms preprocess, 753.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 711.2ms\n",
      "Speed: 3.0ms preprocess, 711.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 713.0ms\n",
      "Speed: 3.0ms preprocess, 713.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 13 persons, 709.8ms\n",
      "Speed: 3.2ms preprocess, 709.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 711.5ms\n",
      "Speed: 3.0ms preprocess, 711.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 720.5ms\n",
      "Speed: 3.1ms preprocess, 720.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 708.6ms\n",
      "Speed: 3.4ms preprocess, 708.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 751.7ms\n",
      "Speed: 3.6ms preprocess, 751.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 751.2ms\n",
      "Speed: 3.1ms preprocess, 751.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 750.9ms\n",
      "Speed: 3.0ms preprocess, 750.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 750.0ms\n",
      "Speed: 3.1ms preprocess, 750.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 756.4ms\n",
      "Speed: 3.1ms preprocess, 756.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 753.8ms\n",
      "Speed: 3.3ms preprocess, 753.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 772.5ms\n",
      "Speed: 3.1ms preprocess, 772.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 775.2ms\n",
      "Speed: 3.2ms preprocess, 775.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 802.6ms\n",
      "Speed: 3.1ms preprocess, 802.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 751.9ms\n",
      "Speed: 3.7ms preprocess, 751.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 761.9ms\n",
      "Speed: 3.5ms preprocess, 761.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 724.1ms\n",
      "Speed: 3.1ms preprocess, 724.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 784.7ms\n",
      "Speed: 3.3ms preprocess, 784.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 760.4ms\n",
      "Speed: 3.7ms preprocess, 760.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 740.7ms\n",
      "Speed: 3.0ms preprocess, 740.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 774.4ms\n",
      "Speed: 7.2ms preprocess, 774.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 819.9ms\n",
      "Speed: 3.1ms preprocess, 819.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 768.6ms\n",
      "Speed: 3.1ms preprocess, 768.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 775.1ms\n",
      "Speed: 3.1ms preprocess, 775.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 822.8ms\n",
      "Speed: 3.2ms preprocess, 822.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 790.4ms\n",
      "Speed: 3.5ms preprocess, 790.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 723.7ms\n",
      "Speed: 3.1ms preprocess, 723.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 715.2ms\n",
      "Speed: 3.7ms preprocess, 715.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 733.1ms\n",
      "Speed: 3.5ms preprocess, 733.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 775.9ms\n",
      "Speed: 3.1ms preprocess, 775.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 760.6ms\n",
      "Speed: 3.2ms preprocess, 760.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 740.2ms\n",
      "Speed: 3.1ms preprocess, 740.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 764.2ms\n",
      "Speed: 3.1ms preprocess, 764.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 737.5ms\n",
      "Speed: 3.1ms preprocess, 737.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 707.4ms\n",
      "Speed: 3.5ms preprocess, 707.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 712.5ms\n",
      "Speed: 3.1ms preprocess, 712.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 774.9ms\n",
      "Speed: 3.1ms preprocess, 774.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 756.6ms\n",
      "Speed: 3.5ms preprocess, 756.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 17 persons, 771.9ms\n",
      "Speed: 3.1ms preprocess, 771.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 748.1ms\n",
      "Speed: 3.1ms preprocess, 748.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 726.3ms\n",
      "Speed: 3.2ms preprocess, 726.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 766.0ms\n",
      "Speed: 3.1ms preprocess, 766.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 741.0ms\n",
      "Speed: 3.1ms preprocess, 741.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 762.1ms\n",
      "Speed: 3.5ms preprocess, 762.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 774.3ms\n",
      "Speed: 3.1ms preprocess, 774.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 758.2ms\n",
      "Speed: 3.1ms preprocess, 758.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 739.7ms\n",
      "Speed: 3.1ms preprocess, 739.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 746.1ms\n",
      "Speed: 3.0ms preprocess, 746.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 15 persons, 721.2ms\n",
      "Speed: 3.0ms preprocess, 721.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 705.4ms\n",
      "Speed: 3.1ms preprocess, 705.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 740.0ms\n",
      "Speed: 3.5ms preprocess, 740.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 751.2ms\n",
      "Speed: 3.6ms preprocess, 751.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 748.8ms\n",
      "Speed: 3.8ms preprocess, 748.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 20 persons, 744.9ms\n",
      "Speed: 3.0ms preprocess, 744.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 20 persons, 781.0ms\n",
      "Speed: 3.1ms preprocess, 781.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 18 persons, 750.1ms\n",
      "Speed: 3.8ms preprocess, 750.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 747.3ms\n",
      "Speed: 3.5ms preprocess, 747.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 19 persons, 754.2ms\n",
      "Speed: 3.6ms preprocess, 754.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 16 persons, 756.3ms\n",
      "Speed: 3.1ms preprocess, 756.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "\n",
      "0: 640x1280 14 persons, 763.8ms\n",
      "Speed: 3.0ms preprocess, 763.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 1280)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your classes here (same order as during YOLO training)\n",
    "classes = [\"pedestrians\", \"bikes\", \"bicyclists\", \"e-scooters\", \"e-scooterists\"]\n",
    "\n",
    "name_model=\"yolo11x\"\n",
    "# Load YOLO model\n",
    "model = YOLO(\"algos/\"+name_model+\".pt\")\n",
    "\n",
    "video_name='Test-360'\n",
    "# Open the video file\n",
    "video_path = \"Videos/\" + video_name + \".mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video frame dimensions and fps\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "images_dir = \"yolo/datasets_\"+ video_name +\"/one/images\"\n",
    "labels_dir = \"yolo/datasets_\"+ video_name +\"/one/labels\"\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "# Create the classes.txt file\n",
    "classes_file = \"yolo/datasets_\"+ video_name +\"/one/classes.txt\"\n",
    "with open(classes_file, \"w\") as class_f:\n",
    "    for cls in classes:\n",
    "        class_f.write(f\"{cls}\\n\")\n",
    "\n",
    "print(f\"Classes file created at: {classes_file}\")\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "output_path = \"predictions/annotated_video_\"+ video_name + '_'+name_model +\".mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "frame_idx = 0  # Frame counter\n",
    "\n",
    "# Process video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Run YOLO detection\n",
    "    results = model.track(frame, persist=True, imgsz=1280, conf=0.3, classes=[0, 1])\n",
    "\n",
    "    # Visualize results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Save the current frame as an image for Label Studio\n",
    "    image_file = os.path.join(images_dir, f\"frame_{frame_idx:04d}.jpg\")\n",
    "    cv2.imwrite(image_file, frame)\n",
    "\n",
    "    # Save detections in YOLO format\n",
    "    label_file = os.path.join(labels_dir, f\"frame_{frame_idx:04d}.txt\")\n",
    "    with open(label_file, \"w\") as f:\n",
    "        for r in results[0].boxes:\n",
    "            cls = int(r.cls)\n",
    "            x_center, y_center, w, h = r.xywhn[0]  # normalized (x_center, y_center, width, height)\n",
    "\n",
    "            # Write to YOLO label file\n",
    "            f.write(f\"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "\n",
    "          \n",
    "      \n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    # Display the annotated frame (optional, can comment out)\n",
    "    # cv2.imshow(\"Annotated Frame\", annotated_frame)\n",
    "\n",
    "    # Break loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "    frame_idx += 1  # Increment frame counter\n",
    "\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Dfinition du dataset\n",
    "dataset = {\n",
    "    \"train\": \"/Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/train\",\n",
    "    \"val\": \"/Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/val\",\n",
    "    \"nc\": 5,  # Nombre de classes\n",
    "    \"names\": ['pedestrians', 'bikes', 'bicyclists', 'e-scooterists', 'e-scooters'] # Classes\n",
    "}\n",
    "\n",
    "# Sauvegarde dans un fichier\n",
    "with open(\"dataset.yaml\", \"w\") as file:\n",
    "    yaml.dump(dataset, file, default_flow_style=False)\n",
    "\n",
    "print(\"Fichier dataset.yaml cr avec succs !\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.94 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.87  Python-3.11.11 torch-2.6.0 CPU (Apple M3 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=algos/MicroVision-Chalmers-x-20250304.pt, data=dataset.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=1280, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
      "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
      "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
      "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
      "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
      "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
      " 10                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2   1700352  ultralytics.nn.modules.block.C3k2            [1536, 384, 2, True]          \n",
      " 17                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   5317632  ultralytics.nn.modules.block.C3k2            [1152, 768, 2, True]          \n",
      " 20                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
      " 23        [16, 19, 22]  1   3151327  ultralytics.nn.modules.head.Detect           [5, [384, 768, 768]]          \n",
      "YOLO11x summary: 357 layers, 56,879,551 parameters, 56,879,535 gradients, 195.5 GFLOPs\n",
      "\n",
      "Transferred 180/1015 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/martin.dejaeghere/Microsint2/yolo/datasets_GS010646/one/labels... 2029 images, 10 backgrounds, 0 corrupt: 100%|| 2039/2039 [00:00<00:00, 2909.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/martin.dejaeghere/Microsint2/yolo/datasets_GS010646/one/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/val/labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|| 46/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[198]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1280\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/engine/model.py:810\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    807\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    809\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:208\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    205\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:381\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m.amp):\n\u001b[32m    380\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.preprocess_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     \u001b[38;5;28mself\u001b[39m.loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n\u001b[32m    383\u001b[39m         \u001b[38;5;28mself\u001b[39m.loss *= world_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:113\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    101\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:291\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcriterion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m.criterion = \u001b[38;5;28mself\u001b[39m.init_criterion()\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.criterion(preds, batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:114\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:132\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:153\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    154\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:239\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:239\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m y.extend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:264\u001b[39m, in \u001b[36mC3.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    263\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv3(torch.cat((\u001b[38;5;28mself\u001b[39m.m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28mself\u001b[39m.cv2(x)), \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:51\u001b[39m, in \u001b[36mConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28mself\u001b[39m.bn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = model.train(data=\"dataset.yaml\", epochs=100, imgsz=1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.87  Python-3.11.11 torch-2.6.0 CPU (Apple M3 Pro)\n",
      "YOLO11x summary (fused): 190 layers, 56,832,799 parameters, 0 gradients, 194.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|| 46/46 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [01:54<00:00, 38.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46        563      0.908      0.636      0.794      0.665\n",
      "                person         46        420       0.88       0.54      0.733      0.581\n",
      "               bicycle         23         67      0.927      0.567       0.76      0.629\n",
      "               cyclist         21         28      0.769      0.714      0.805       0.71\n",
      "             e-scooter         20         39      0.964      0.692      0.841      0.717\n",
      "          e-scooterist          8          9          1      0.667      0.833      0.689\n",
      "Speed: 1.3ms preprocess, 2453.7ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val22\u001b[0m\n",
      "0.6653092581562342\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Validate with a custom dataset\n",
    "name_model=\"MicroVision-Chalmers-x-distorted-20250318\"\n",
    "# Load YOLO model\n",
    "model = YOLO(\"algos/\"+name_model+\".pt\")\n",
    "\n",
    "metrics = model.val(data=\"dataset.yaml\",conf=0.4) \n",
    "print(metrics.box.map)  # map50-95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.87  Python-3.11.11 torch-2.6.0 CPU (Apple M3 Pro)\n",
      "YOLO11x summary (fused): 190 layers, 56,832,799 parameters, 0 gradients, 194.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|| 46/46 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [01:54<00:00, 38.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46        563      0.855      0.432      0.668      0.505\n",
      "                person         46        420      0.981      0.252      0.617      0.432\n",
      "               bicycle         23         67      0.875      0.313      0.611      0.438\n",
      "               cyclist         21         28       0.75      0.536      0.676      0.567\n",
      "             e-scooter         20         39          1      0.615      0.808      0.581\n",
      "          e-scooterist          8          9      0.667      0.444       0.63      0.506\n",
      "Speed: 1.4ms preprocess, 2453.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val23\u001b[0m\n",
      "0.5047384110658028\n"
     ]
    }
   ],
   "source": [
    "name_model=\"MicroVision-Chalmers-x-only-distorted-20250320\"\n",
    "model=YOLO(\"algos/\"+name_model+\".pt\")\n",
    "\n",
    "# Validate with a custom dataset\n",
    "metrics = model.val(data=\"dataset.yaml\",conf=0.4)  \n",
    "print(metrics.box.map)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.87  Python-3.11.11 torch-2.6.0 CPU (Apple M3 Pro)\n",
      "YOLO11x summary (fused): 190 layers, 56,832,799 parameters, 0 gradients, 194.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/test/labels... 46 images, 0 backgrounds, 0 corrupt: 100%|| 46/46 [00:00<00:00, 3116.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [01:52<00:00, 37.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46        563      0.792      0.739      0.832      0.758\n",
      "                person         46        420       0.95      0.583      0.777      0.706\n",
      "               bicycle         23         67      0.939      0.687      0.829      0.731\n",
      "               cyclist         21         28      0.714      0.714      0.804        0.8\n",
      "             e-scooter         20         39      0.914      0.821      0.888      0.874\n",
      "          e-scooterist          8          9      0.444      0.889      0.863       0.68\n",
      "Speed: 2.4ms preprocess, 2417.6ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val31\u001b[0m\n",
      "0.758364834795386\n"
     ]
    }
   ],
   "source": [
    "name_model=\"MicroVision-Chalmers-x-20250304\"\n",
    "model=YOLO(\"algos/\"+name_model+\".pt\")\n",
    "\n",
    "# Validate with a custom dataset\n",
    "metrics = model.val(data=\"dataset.yaml\",conf=0.4)  # output optional\n",
    "print(metrics.box.map)  # map50-95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier dataset.yaml cr avec succs !\n",
      "Ultralytics 8.3.87  Python-3.11.11 torch-2.6.0 CPU (Apple M3 Pro)\n",
      "YOLO11x summary (fused): 190 layers, 56,919,424 parameters, 0 gradients, 194.9 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|| 46/46 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [01:56<00:00, 38.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46        487      0.699      0.568        0.6      0.465\n",
      "                person         46        420      0.772      0.614      0.657      0.492\n",
      "               bicycle         23         67      0.625      0.522      0.544      0.438\n",
      "Speed: 1.4ms preprocess, 2494.2ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val19\u001b[0m\n",
      "0.46490512603597517\n"
     ]
    }
   ],
   "source": [
    "# Dfinition du dataset\n",
    "dataset = {\n",
    "    \"train\": \"/Users/martin.dejaeghere/Microsint2/yolo/datasets_GS010646\",\n",
    "    \"val\": \"/Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie\",\n",
    "    \"nc\": 2,  # Nombre de classes\n",
    "    \"names\": ['persons', 'bikes'] # Classes\n",
    "}\n",
    "\n",
    "# Sauvegarde dans un fichier\n",
    "with open(\"dataset.yaml\", \"w\") as file:\n",
    "    yaml.dump(dataset, file, default_flow_style=False)\n",
    "\n",
    "print(\"Fichier dataset.yaml cr avec succs !\")\n",
    "\n",
    "name_model=\"yolo11x\"\n",
    "model=YOLO(\"algos/\"+name_model+\".pt\")\n",
    "\n",
    "# Validate with a custom dataset\n",
    "metrics = model.val(data=\"dataset.yaml\",conf=0.4,imgsz=1280,classes=[0, 1])  \n",
    "print(metrics.box.map)  # map50-95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path='/Users/martin.dejaeghere/Microsint2/algos/MicroVision-Chalmers-x-distorted-20250318.pt'\n",
    "yaml_path='/Users/martin.dejaeghere/Microsint2/dataset.yaml'\n",
    "val_path='/Users/martin.dejaeghere/Microsint2/runs/detect'\n",
    "data_path='/Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Run validation ...\n",
      "Ultralytics 8.3.87  Python-3.11.11 torch-2.6.0 CPU (Apple M3 Pro)\n",
      "YOLO11x summary (fused): 190 layers, 56,832,799 parameters, 0 gradients, 194.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/martin.dejaeghere/Microsint2/yolo/datasets_tolabel_copie/one/val/labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|| 46/46 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [01:56<00:00, 38.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         46        563      0.864      0.646      0.776      0.617\n",
      "                person         46        420      0.877      0.578       0.73      0.528\n",
      "               bicycle         23         67      0.906      0.582      0.772      0.585\n",
      "               cyclist         21         28        0.7      0.679       0.76      0.663\n",
      "             e-scooter         20         39      0.904      0.727      0.842      0.676\n",
      "          e-scooterist          8          9      0.934      0.667      0.777      0.635\n",
      "Speed: 1.4ms preprocess, 2502.6ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Saving runs/detect/detect/predictions.json...\n",
      "Results saved to \u001b[1mruns/detect/detect\u001b[0m\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/martin.dejaeghere/Microsint2/runs/detect/predictions.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[197]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcoco_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magnostic_nms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/val.py:141\u001b[39m, in \u001b[36mcoco_eval\u001b[39m\u001b[34m(model_path, yaml_path, val_path, data_path, thsd_small, thsd_medium, split, iou, agnostic_nms)\u001b[39m\n\u001b[32m    138\u001b[39m anno = COCO(annotations_json)\n\u001b[32m    139\u001b[39m imgIds = \u001b[38;5;28msorted\u001b[39m(anno.getImgIds())\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m pred = \u001b[43manno\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadRes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m val = COCOeval(anno, pred, \u001b[33m\"\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m val.params.imgIds = imgIds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Microsint2/.conda/lib/python3.11/site-packages/pycocotools/coco.py:319\u001b[39m, in \u001b[36mCOCO.loadRes\u001b[39m\u001b[34m(self, resFile)\u001b[39m\n\u001b[32m    317\u001b[39m tic = time.time()\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(resFile) == \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (PYTHON_VERSION == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(resFile) == unicode):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resFile) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    320\u001b[39m         anns = json.load(f)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(resFile) == np.ndarray:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/martin.dejaeghere/Microsint2/runs/detect/predictions.json'"
     ]
    }
   ],
   "source": [
    "coco_eval(model_path, yaml_path, val_path, data_path, 128, 288, split=\"val\", iou=0.45, agnostic_nms=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
