{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ultralytics\n",
    "model = torch.load('MicroVision-Chalmers-x-20250304.pt', weights_only=False)\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from ultralytics import YOLO  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1273976817.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mclasses = [\"pedestrians\", \"bikes\", \"bicyclists\", \"e-scooters\", \"e-scooterists\"]w\u001b[39m\n                                                                                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your classes here (same order as during YOLO training)\n",
    "classes = [\"pedestrians\", \"bikes\", \"bicyclists\", \"e-scooters\", \"e-scooterists\"]\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"MicroVision-Chalmers-x-20250304.pt\")\n",
    "\n",
    "video_name='GS010659'\n",
    "# Open the video file\n",
    "video_path = \"Videos/\" + video_name + \".mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video frame dimensions and fps\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "images_dir = \"yolo/datasets_\"+ video_name +\"/one/images\"\n",
    "labels_dir = \"yolo/datasets_\"+ video_name +\"/one/labels\"\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "# Create the classes.txt file\n",
    "classes_file = \"yolo/datasets_\"+ video_name +\"/one/classes.txt\"\n",
    "with open(classes_file, \"w\") as class_f:\n",
    "    for cls in classes:\n",
    "        class_f.write(f\"{cls}\\n\")\n",
    "\n",
    "print(f\"Classes file created at: {classes_file}\")\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "output_path = \"predictions/annotated_video_\"+ video_name +\".mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "frame_idx = 0  # Frame counter\n",
    "\n",
    "# Process video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Run YOLO detection\n",
    "    results = model.track(frame, persist=True, imgsz=1280,conf=0.1)\n",
    "\n",
    "    # Visualize results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Save the current frame as an image for Label Studio\n",
    "    image_file = os.path.join(images_dir, f\"frame_{frame_idx:04d}.jpg\")\n",
    "    cv2.imwrite(image_file, frame)\n",
    "\n",
    "    # Save detections in YOLO format\n",
    "    label_file = os.path.join(labels_dir, f\"frame_{frame_idx:04d}.txt\")\n",
    "    with open(label_file, \"w\") as f:\n",
    "        for r in results[0].boxes:\n",
    "            cls = int(r.cls)\n",
    "            x_center, y_center, w, h = r.xywhn[0]  # normalized (x_center, y_center, width, height)\n",
    "\n",
    "            # Write to YOLO label file\n",
    "            f.write(f\"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "\n",
    "          \n",
    "      \n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    # Display the annotated frame (optional, can comment out)\n",
    "    # cv2.imshow(\"Annotated Frame\", annotated_frame)\n",
    "\n",
    "    # Break loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "    frame_idx += 1  # Increment frame counter\n",
    "\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier dataset.yaml crÃ©Ã© avec succÃ¨s !\n",
      "Ultralytics 8.3.87 ðŸš€ Python-3.11.11 torch-2.6.0 CPU (Apple M3 Pro)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/martin.dejaeghere/Microsint2/val/labels.cache... 56 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:37<00:00, 39.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56       1673      0.769      0.258      0.515      0.456\n",
      "                person         56       1320      0.979      0.145      0.563      0.519\n",
      "               bicycle         34         36          0          0          0          0\n",
      "               cyclist         53        163      0.866      0.356      0.619      0.586\n",
      "             e-scooter         43         74          1      0.162      0.581      0.397\n",
      "          e-scooterist         53         80          1      0.625      0.812      0.779\n",
      "Speed: 1.4ms preprocess, 2789.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val13\u001b[0m\n",
      "0.456353859799755\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# DÃ©finition du dataset\n",
    "dataset = {\n",
    "    \"train\": \"//Users/martin.dejaeghere/Microsint2/yolo/datasets/one\",\n",
    "    \"val\": \"/Users/martin.dejaeghere/Microsint2/val\",\n",
    "    \"nc\": 5,  # Nombre de classes\n",
    "    \"names\": ['pedestrians', 'bikes', 'bicyclists', 'e-scooterists', 'e-scooters']  # Classes\n",
    "}\n",
    "\n",
    "# Sauvegarde dans un fichier\n",
    "with open(\"dataset.yaml\", \"w\") as file:\n",
    "    yaml.dump(dataset, file, default_flow_style=False)\n",
    "\n",
    "print(\"Fichier dataset.yaml crÃ©Ã© avec succÃ¨s !\")\n",
    "\n",
    "\n",
    "# Validate with a custom dataset\n",
    "metrics = model.val(data=\"dataset.yaml\",conf=0.7)  # output optional\n",
    "print(metrics.box.map)  # map50-95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix.plot()  # normalized confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
